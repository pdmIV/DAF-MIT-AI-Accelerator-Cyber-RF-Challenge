Given the provided chart, Random Forest was the best model architecture to train with. I used XGBoost and compared it to Random Forest.
XGBoost and Random Forest are similar, but XGBoost has some critical advantages:

1) It generally has higher accuracy due to error correction (gradient boosting)
2) It has less of a chance of overfitting, improving generalization
3) It is optimized for speed and memory usage, cutting resource costs for training and usage
4) It can natively handle missing training data, making it more robust

Here is the reference chart I mentioned earlier:
![alt text](<performance chart.jpg>)